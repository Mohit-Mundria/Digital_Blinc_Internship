{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d66e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a969a42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A transformer model is a type of neural network architecture widely used in Natural Language Processing (NLP) tasks, particularly for sequence-to-sequence models such as machine translation, text summarization, and question-answering.\n",
      "\n",
      "The Transformer model was first introduced by Vaswani et al. in 2017, replacing the traditional Recurrent Neural Network (RNN) architecture with a new paradigm called Self-Attention Mechanism.\n",
      "\n",
      "**Key Components of Transformer Model:**\n",
      "\n",
      "1. **Self-Attention Mechanism:** The core component of the Transformer model is the self-attention mechanism. It allows the model to weigh the importance of different words in a sentence relative to each other, enabling it to capture long-range dependencies and contextual relationships between tokens.\n",
      "2. **Encoder-Decoder Architecture:** The Transformer model consists of two main components: an encoder and a decoder. The encoder takes in a sequence of input tokens (e.g., words or characters) and generates a sequence of output tokens.\n",
      "3. **Multi-Head Attention:** The self-attention mechanism is applied to the input sequence multiple times, using different attention heads. Each head attends to different parts of the input sequence independently.\n",
      "\n",
      "**How Transformer Model Works:**\n",
      "\n",
      "1. Input sequence is passed through an embedding layer, which converts each token into a dense vector representation.\n",
      "2. The output of the embedding layer is fed into the encoder layer, where self-attention and feed-forward neural network (FFNN) layers are applied multiple times.\n",
      "3. The output of the FFNN layers is then passed to the decoder layer, which generates the output sequence by applying another set of self-attention and FFNN layers.\n",
      "\n",
      "**Advantages of Transformer Model:**\n",
      "\n",
      "1. **Parallelization:** The Transformer model can be parallelized more easily than RNNs, as it only requires a single pass through the input sequence.\n",
      "2. **Scalability:** The model's ability to capture long-range dependencies makes it well-suited for handling longer sequences and larger datasets.\n",
      "3. **Improved Performance:** The Transformer model has achieved state-of-the-art performance on various NLP tasks, such as machine translation and text classification.\n",
      "\n",
      "**Popular Variants of Transformer Model:**\n",
      "\n",
      "1. BERT (Bidirectional Encoder Representations from Transformers): a pre-trained language model that uses the Transformer architecture to learn contextualized representations of words.\n",
      "2. RoBERTa (Robustly Optimized BERT Pretraining Approach): an improved version of BERT with some modifications to the training procedure.\n",
      "\n",
      "The Transformer model has revolutionized the field of NLP, enabling state-of-the-art performance on a wide range of tasks and applications. Its ability to capture long-range dependencies and contextual relationships makes it a popular choice for many NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    'http://localhost:11434/api/generate',\n",
    "    json={\n",
    "        'model': 'llama3.2:latest',\n",
    "        'prompt': 'Explain what a transformer model is in NLP.',\n",
    "        'stream': False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json()['response'])\n",
    "#print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a00c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\miniconda3\\envs\\venv2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e012bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_response():\n",
    "    User_input=input(\"Enter the Text to predict the Emoji from LLAMA 3.2 Model: \")\n",
    "    \n",
    "    response = requests.post(\n",
    "         'http://localhost:11434/api/generate',\n",
    "         json={\n",
    "            'model': 'llama3.2:latest',\n",
    "            'prompt': f\"Predict an emoji that best represents this phrase: {User_input}\",\n",
    "            'stream': False\n",
    "        }\n",
    "    )\n",
    "    return response.json()['response']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
